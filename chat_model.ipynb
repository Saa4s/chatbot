{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gCrI4eShjFv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/data/chatbot/dialogs.txt', 'r') as f:\n",
        "  lines = [i.strip() for i in f]\n",
        "# questions = np.array(lines[:-1])\n",
        "questions = np.array([line.split() for line in lines[:-1]])\n",
        "# answers = np.array(list(map(lambda x: f'<bos> {x} <eos>', lines[1:])))\n",
        "answers = np.array([(f'<bos> {line} <eos>').split() for line in lines[1:]])"
      ],
      "metadata": {
        "id": "Ua6QMAlJhyll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9885e8-77dd-4e97-8a68-7f84d61dce30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-cea66cb0a902>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  questions = np.array([line.split() for line in lines[:-1]])\n",
            "<ipython-input-5-cea66cb0a902>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  answers = np.array([(f'<bos> {line} <eos>').split() for line in lines[1:]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 5000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size,  oov_token = 'unk')\n",
        "tokenizer.fit_on_texts(answers[:1001])\n",
        "vocabluary_items = list(tokenizer.word_index.items())\n",
        "vocabulary_size = len(vocabluary_items) + 1\n",
        "\n",
        "print(vocabluary_items[:100])\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cQeKkZZiUdO",
        "outputId": "b2e7dcef-3a74-4206-a2ae-dcc2904f5dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('unk', 1), ('<bos>', 2), ('<eos>', 3), (',', 4), ('?', 5), ('.', 6), ('я', 7), ('а', 8), ('и', 9), ('у', 10), ('в', 11), ('не', 12), ('ты', 13), ('люблю', 14), ('на', 15), ('!', 16), ('меня', 17), ('как', 18), ('это', 19), ('что', 20), ('очень', 21), ('с', 22), ('привет', 23), ('да', 24), ('мне', 25), ('есть', 26), ('но', 27), ('тоже', 28), ('тебя', 29), ('чем', 30), ('вы', 31), ('вот', 32), ('вас', 33), ('дела', 34), ('нет', 35), ('по', 36), ('хорошо', 37), ('о', 38), ('любишь', 39), ('зовут', 40), ('так', 41), ('то', 42), ('к', 43), ('время', 44), ('ну', 45), ('животных', 46), ('нравится', 47), ('работаю', 48), ('за', 49), ('живу', 50), ('круто', 51), ('из', 52), ('ещ', 53), ('может', 54), ('работаешь', 55), ('тебе', 56), ('занимаешься', 57), ('или', 58), ('много', 59), ('уже', 60), ('хочу', 61), ('приятно', 62), ('лет', 63), ('сейчас', 64), ('их', 65), ('пока', 66), ('когда', 67), ('кем', 68), ('была', 69), ('себе', 70), ('вообще', 71), ('где', 72), ('здорово', 73), ('учусь', 74), ('думаю', 75), ('поэтому', 76), ('кстати', 77), ('могу', 78), ('рок', 79), ('музыку', 80), ('нас', 81), ('только', 82), ('свободное', 83), ('со', 84), ('хобби', 85), ('все', 86), ('готовить', 87), ('собак', 88), ('кто', 89), ('спасибо', 90), ('просто', 91), ('обожаю', 92), ('работу', 93), ('от', 94), ('еще', 95), ('твои', 96), ('там', 97), ('конечно', 98), ('можно', 99), ('они', 100)]\n",
            "1750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.items())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYGj_XOFSldD",
        "outputId": "70b0228f-a20e-4718-8fed-3e810c375e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('unk', 1),\n",
              " ('<bos>', 2),\n",
              " ('<eos>', 3),\n",
              " (',', 4),\n",
              " ('?', 5),\n",
              " ('.', 6),\n",
              " ('я', 7),\n",
              " ('а', 8),\n",
              " ('и', 9),\n",
              " ('у', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['<bos> привет как дела ?'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBg2KPgTSkBt",
        "outputId": "58f58ef7-657b-49e1-acb5-270d419db5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 23, 18, 34]]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256 # скрытая размерность\n",
        "num_samples = 10000\n",
        "embedding_dim = 256  # Размерность эмбеддинга\n",
        "# max_len = 20"
      ],
      "metadata": {
        "id": "Ub51ITshh6vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(phrases):\n",
        "  tokenized = tokenizer.texts_to_sequences(phrases)\n",
        "  max_len = max([ len(x) for x in tokenized])\n",
        "  # max_len = 20\n",
        "\n",
        "  padded = pad_sequences(tokenized, maxlen=max_len, padding='post')\n",
        "  encoded = np.array(padded)\n",
        "  return encoded, max_len"
      ],
      "metadata": {
        "id": "gJQ9nGRPjFN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input, max_len_quest = prepare_data(questions[:1000])"
      ],
      "metadata": {
        "id": "BkJfrNljjRMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phrases = answers[:1001][:-1]\n",
        "phrases = [line[:-1] for line in answers[:1001]]\n",
        "tokenized = tokenizer.texts_to_sequences(phrases)\n",
        "max_len_answ = max([ len(x) for x in tokenized])\n",
        "decoder_input = pad_sequences(tokenized, maxlen=max_len_answ, padding='post')"
      ],
      "metadata": {
        "id": "gwNFBLsWMDzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phrases = answers[:1001][1:]\n",
        "phrases = [line[1:] for line in answers[:1001]]\n",
        "tokenized = tokenizer.texts_to_sequences(phrases)\n",
        "padded = pad_sequences(tokenized, maxlen=max_len_answ, padding='post')\n",
        "decoder_output = utils.to_categorical(padded, vocabulary_size)"
      ],
      "metadata": {
        "id": "o4YAzMLwkMjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras import utils\n"
      ],
      "metadata": {
        "id": "3zTEBBqZlO8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(max_len_quest))\n",
        "encoder_embedding = Embedding(vocabulary_size, embedding_dim)(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(max_len_answ))\n",
        "decoder_embedding = Embedding(vocabulary_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocabulary_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "XPwAMb-OlT8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "# print(model.layers[-1].input)\n",
        "print(model.layers[-3].output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE27iqG0mJEQ",
        "outputId": "de4bade6-b5f6-4aef-ebf7-1893b5035357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 62)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 60, 256)      446208      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 62, 256)      446208      ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, 256),        525312      ['embedding_5[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, 62, 256),    525312      ['embedding_6[0][0]',            \n",
            "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 62, 1743)     447951      ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,390,991\n",
            "Trainable params: 2,390,991\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_6')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_6')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_6')>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input, decoder_input], decoder_output,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "6eVd_Jw1mb88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                      [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "rw3jyUTZpIJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer.texts_to_sequences(['<eos> привет как жизнь'])\n",
        "print(tokenized)\n",
        "a = tokenizer.sequences_to_texts(tokenized)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "79jFHb7GrW_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(target_seq, max_len):\n",
        "  tokenized = tokenizer.texts_to_sequences(target_seq)\n",
        "  padded = pad_sequences(tokenized, maxlen=max_len, padding='post')\n",
        "  return np.array(padded)"
      ],
      "metadata": {
        "id": "ekFOggs1qWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = ['<bos>']\n",
        "input_seq = encode(input_seq, max_len_quest)\n",
        "states_value = encoder_model.predict(input_seq)\n",
        "target_seq = encode(['<bos>'], max_len_answ)\n",
        "stop_condition = False\n",
        "decoded_sentence = ''\n",
        "while not stop_condition:\n",
        "  output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "  sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "  sampled_char = tokenizer.sequences_to_texts(str(sampled_token_index))\n",
        "  decoded_sentence += str(sampled_char)\n",
        "  if (sampled_char == '[START]' or len(decoded_sentence) > max_len_answ):\n",
        "          stop_condition = True\n",
        "  states_value = [h, c]\n",
        "  target_seq = encode([sampled_char], max_len_answ)"
      ],
      "metadata": {
        "id": "VwgWIQuEs1OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence"
      ],
      "metadata": {
        "id": "YKbcQGqCRy6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivVIlYTVSAmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXQHN6gMSbU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}